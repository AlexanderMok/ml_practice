{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 高斯分布型：用于classification问题，假定属性/特征是服从正态分布的。\n",
    "- 多项式型：用于离散值模型里。比如文本分类问题里面我们提到过，我们不光看词语是否在文本中出现，也得看出现的次数。如果总词数为n，出现词数为m的话，说起来有点像掷骰子n次出现m次这个词的场景。\n",
    "- 伯努利型：这种情况下，就如之前博文里提到的bag of words处理方式一样，最后得到的特征只有0(没出现)和1(出现过)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# iris dataset\n",
    "iris = datasets.load_iris()\n",
    "iris.data[:5]\n",
    "\n",
    "# supposing that sepal length, sepal width, petal length, petal width are complied with Gaussian Distribution\n",
    "# 4个量独立且服从高斯分布，用贝叶斯分类器建模\n",
    "# Gaussian Model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train\n",
    "clf = gnb.fit(iris.data, iris.target)\n",
    "\n",
    "# predict\n",
    "y_pred = clf.predict(iris.data)\n",
    "\n",
    "right_num = (iris.target == y_pred).sum()\n",
    "\n",
    "#print \"Total num of test data : {:d} , naive bayes accuracy : {:.2f}%\".format(iris.data.shape[0], float(right_num)/iris.data.shape[0])\n",
    "\n",
    "#assert 'Total num of test data :150 , naive bayes accuracy :0.96%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#0:晴 1:阴 2:降水 3:多云\n",
    "data_table = [[\"date\", \"weather\"],\n",
    "              [1, 0],\n",
    "              [2, 1],\n",
    "              [3, 2],\n",
    "              [4, 1],\n",
    "              [5, 2],\n",
    "              [6, 0],\n",
    "              [7, 0],\n",
    "              [8, 3],\n",
    "              [9, 1],\n",
    "              [10, 1]]\n",
    "#当天的天气\n",
    "X = [[0], [1], [2], [1], [2], [0], [0], [3], [1]]\n",
    "\n",
    "#当天的天气对应后一天的天气\n",
    "Y = [1, 2, 1, 2, 0, 0, 3, 1, 1]\n",
    "\n",
    "# X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "# Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "#现在我们把训练数据，和对应的分类放入分类器中进行训练\n",
    "clf = GaussianNB().fit(X, Y)\n",
    "\n",
    "#预测当天是晴天后一天的天气\n",
    "p = [[1]]\n",
    "print clf.predict(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping relationships\n",
    "\n",
    "* The following process compute data of mapping relationships provided by Sogou\n",
    "[DataSet](http://www.sogou.com/labs/dl/cs.html)(in Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 18 labels in the dataset.\n",
      "总共 18 个新闻类别/标签\n",
      "Labels are: \n",
      "[u'IT', u'\\u4f53\\u80b2', u'\\u5065\\u5eb7', u'\\u519b\\u4e8b', u'\\u56fd\\u5185', u'\\u56fd\\u9645', u'\\u5965\\u8fd0', u'\\u5973\\u6027', u'\\u5a31\\u4e50', u'\\u623f\\u4ea7', u'\\u62db\\u8058', u'\\u6559\\u80b2', u'\\u6587\\u5316', u'\\u65c5\\u6e38', u'\\u6821\\u56ed', u'\\u6c7d\\u8f66', u'\\u793e\\u4f1a', u'\\u8d22\\u7ecf'].\n",
      "[['url', 'label']]\n",
      "labels in number: {u'\\u56fd\\u5185': 4, u'\\u6587\\u5316': 12, u'\\u8d22\\u7ecf': 17, u'\\u793e\\u4f1a': 16, u'\\u5a31\\u4e50': 8, u'\\u4f53\\u80b2': 1, u'\\u62db\\u8058': 10, u'\\u6821\\u56ed': 14, u'IT': 0, u'\\u56fd\\u9645': 5, u'\\u623f\\u4ea7': 9, u'\\u6c7d\\u8f66': 15, u'\\u65c5\\u6e38': 13, u'\\u519b\\u4e8b': 3, u'\\u5065\\u5eb7': 2, u'\\u6559\\u80b2': 11, u'\\u5973\\u6027': 7, u'\\u5965\\u8fd0': 6}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import codecs\n",
    "def url_label_lst():\n",
    "    fhand = codecs.open('url_label.txt', encoding='utf-8').read()\n",
    "    url_label_lst = list()\n",
    "    fhand = fhand.split('\\n')\n",
    "    for i in range(len(fhand)):\n",
    "        ul = fhand[i].split('\\t')\n",
    "        url_label_lst.append(ul)\n",
    "    return url_label_lst\n",
    "\n",
    "def url_map_label(url_label_lst):\n",
    "    dct = {}\n",
    "    labels = []\n",
    "    for i in range(len(lst) - 1):\n",
    "        (url,label) = lst[i]\n",
    "        dct[label] = dct.get(label,1)\n",
    "    for key in dct.keys():\n",
    "        labels.append(key)\n",
    "    labels.sort()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def labels_num_dict(labels_lst):\n",
    "    labels_dict = {}\n",
    "    for i in range(len(labels_lst)):\n",
    "        labels_dict[labels_lst[i]] = i   \n",
    "    return labels_dict\n",
    "\n",
    "lst = url_label_lst()\n",
    "labels = url_map_label(lst)\n",
    "    \n",
    "print 'Total {} labels in the dataset.'.format(len(labels))\n",
    "print '总共 {} 个新闻类别/标签'.format(len(labels))\n",
    "print 'Labels are: \\n{0}.'.format(labels)\n",
    "\n",
    "print [['url','label']]\n",
    "labels_num = labels_num_dict(labels)\n",
    "print 'labels in number: {}'.format(labels_num)\n",
    "#sorted([(v,k) for k,v in labels_num.items()])\n",
    "print labels_num[lst[3][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Text num is 963\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import jieba\n",
    "\n",
    "\n",
    "def replace_label_seg(jsonData, url_label_lst, labels_num):\n",
    "    for i in range(len(jsonData) - 1):\n",
    "        # replace url label\n",
    "        for j in range(len(url_label_lst) - 1):\n",
    "            if url_label_lst[j][0] in str(jsonData[i]['url']):\n",
    "                jsonData[i]['url'] = labels_num[url_label_lst[j][1]]\n",
    "                \n",
    "        # word segement Default Mode 精确模式\n",
    "        # some content are empty and it is a list when transforming into json from xml\n",
    "        if isinstance(jsonData[i]['content'],list) or isinstance(jsonData[i]['contenttitle'],list): \n",
    "            jsonData[i]['content'] = ''\n",
    "            jsonData[i]['contenttitle'] = ''\n",
    "            continue\n",
    "        \n",
    "        seg_con = jieba.cut(jsonData[i]['content'], cut_all=False)\n",
    "        # TODO 还需要去除停用词\n",
    "        jsonData[i]['content'] = ' '.join(seg_con)\n",
    "        \n",
    "        seg_title = jieba.cut(jsonData[i]['contenttitle'], cut_all=False)\n",
    "        jsonData[i]['contenttitle'] = ' '.join(seg_title)\n",
    "    \n",
    "    return jsonData\n",
    "\n",
    "\n",
    "import json\n",
    "# load json data to be cleaned and preprocess\n",
    "data = open(\"data.json\").read()\n",
    "jsonData = json.loads(data)\n",
    "# load labels and transform into number\n",
    "lst = url_label_lst()\n",
    "labels = url_map_label(lst)\n",
    "labels_num = labels_num_dict(labels)\n",
    "\n",
    "# clean and preprocess\n",
    "jsonData = replace_label_seg(jsonData, lst, labels_num)\n",
    "\n",
    "#print jsonData[8]['contenttitle']\n",
    "# save cleaned data\n",
    "#cleaned_data = json.dumps(jsonData, indent = 4)\n",
    "\n",
    "#data_pro = codecs.open('data_processed.json','w', encoding='utf-8')\n",
    "#data_pro.write(cleaned_data)\n",
    "#data_pro.close()\n",
    "\n",
    "print 'Processing completed. Text num is {0}'.format(len(jsonData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The origin data is in xml and GBK format. It is processed into json and utf-8 file via Java\n",
    "\n",
    "### The above process completed following tasks:\n",
    "* The url indicates the label of a text. Mapping relation is provided by Sogou. I replace the url with Chinese Label. See method<code>url_label_lst()</code>\n",
    "* Word segmentation also completed while replacing urls to labels. Jieba is applied to segment\n",
    "* Finally, save the processed data in json and utf-8 format.\n",
    "\n",
    "### Next Step is to futher process and train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print jsonData[3]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#json_proc = codecs.open('data_processed.json','r', encoding='utf-8').read()\n",
    "#fileData = json.loads(json_proc)\n",
    "\n",
    "fileWrite = codecs.open('data_processed3.json','w','utf-8')\n",
    "for i in range(len(jsonData) - 1):\n",
    "    if len(str(jsonData[i]['url'])) > 0 and len(jsonData[i]['content']) > 0 and len(jsonData[i]['contenttitle']) > 0:\n",
    "        fileWrite.write(str(jsonData[i]['url']).strip() + '\\t' + jsonData[i]['content'].strip() + '\\n')\n",
    "\n",
    "fileWrite.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-307-cac306f71e24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainText\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mlable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    }
   ],
   "source": [
    "data2 = open(\"data_processed2.json\").read()\n",
    "data2 = data2.split('\\n')\n",
    "print len(data2)\n",
    "for line in range(len(trainText) - 2):\n",
    "        lable,text = data2[line].split('\\t',1)\n",
    "print lable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'IT', u'\\u4f53\\u80b2', u'\\u5065\\u5eb7', u'\\u519b\\u4e8b', u'\\u56fd\\u5185', u'\\u56fd\\u9645', u'\\u5965\\u8fd0', u'\\u5973\\u6027', u'\\u5a31\\u4e50', u'\\u623f\\u4ea7', u'\\u62db\\u8058', u'\\u6559\\u80b2', u'\\u6587\\u5316', u'\\u65c5\\u6e38', u'\\u6821\\u56ed', u'\\u6c7d\\u8f66', u'\\u793e\\u4f1a', u'\\u8d22\\u7ecf']\n"
     ]
    }
   ],
   "source": [
    "print labels\n",
    "lables = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#总共有9种新闻类别，我们给每个类别一个编号\n",
    "def lable2id(lable):\n",
    "    for i in xrange(len(lables)):\n",
    "        if lable == lables[i]:\n",
    "            return i\n",
    "    raise Exception('Error lable {0}'.format(lable))\n",
    "\n",
    "\n",
    "def doc_dict():\n",
    "    return [0] * len(lables)\n",
    "\n",
    "def mutual_info(N,Nij,Ni_,N_j):\n",
    "    '''\n",
    "        计算互信息，这里log的底取为2\n",
    "    '''\n",
    "    return Nij * 1.0 / N * math.log(N * (Nij+1)*1.0/(Ni_*N_j))/ math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_for_cates(trainText, featureFile):\n",
    "    '''\n",
    "        遍历文件，统计每个词在每个类别出现的次数，和每类的文档数\n",
    "        并写入结果特征文件\n",
    "    '''\n",
    "    docCount = [0] * len(lables)\n",
    "    wordCount = collections.defaultdict(lambda:doc_dict())\n",
    "    #扫描文件和计数\n",
    "    trainText = trainText.split('\\n')\n",
    "    for line in range(len(trainText) - 2):\n",
    "        lable,text = trainText[line].strip().split('\\t',1)\n",
    "        index = lable2id(lable[0])        \n",
    "        words = text.split(' ')\n",
    "        for word in words:\n",
    "            wordCount[word][index] += 1\n",
    "            docCount[index] += 1\n",
    "    #计算互信息值\n",
    "    print \"计算互信息，提取关键/特征词中，请稍后...\"\n",
    "    miDict = collections.defaultdict(doc_dict())\n",
    "    N = sum(docCount)\n",
    "    for k,vs in wordCount.items():\n",
    "        for i in xrange(len(vs)):\n",
    "            N11 = vs[i]\n",
    "            N10 = sum(vs) - N11\n",
    "            N01 = docCount[i] - N11\n",
    "            N00 = N - N11 - N10 - N01\n",
    "            mi = mutual_info(N,N11,N10+N11,N01+N11) + mutual_info(N,N10,N10+N11,N00+N10)+ mutual_info(N,N01,N01+N11,N01+N00)+ mutual_info(N,N00,N00+N10,N00+N01)\n",
    "            miDict[k][i] = mi\n",
    "    fWords = set()\n",
    "    for i in xrange(len(docCount)):\n",
    "        keyf = lambda x:x[1][i]\n",
    "        sortedDict = sorted(miDict.items(),key=keyf,reverse=True)\n",
    "        for j in xrange(100):\n",
    "            fWords.add(sortedDict[j][0])\n",
    "    out = open(featureFile, 'w')\n",
    "    #输出各个类的文档数目\n",
    "    out.write(str(docCount)+\"\\n\")\n",
    "    #输出互信息最高的词作为特征词\n",
    "    for fword in fWords:\n",
    "        out.write(fword+\"\\n\")\n",
    "    print \"特征词写入完毕...\"\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_feature_words(featureFile):\n",
    "    '''\n",
    "        从特征文件导入特征词\n",
    "    '''\n",
    "    f = open(featureFile)\n",
    "    #各个类的文档数目\n",
    "    docCounts = eval(f.readline())\n",
    "    features = set()\n",
    "    #读取特征词\n",
    "    for line in f:\n",
    "        features.add(line.strip())\n",
    "    f.close()\n",
    "    return docCounts,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bayes(featureFile, textFile, modelFile):\n",
    "    '''\n",
    "        训练贝叶斯模型，实际上计算每个类中特征词的出现次数\n",
    "    '''\n",
    "    print \"使用朴素贝叶斯训练中...\"\n",
    "    docCounts,features = load_feature_words(featureFile)\n",
    "    wordCount = collections.defaultdict(doc_dict())\n",
    "    #每类文档特征词出现的次数\n",
    "    tCount = [0]*len(docCounts)\n",
    "    for line in open(textFile):\n",
    "        lable,text = line.strip().split('\\t',1)\n",
    "        index = lable2id(lable[0])        \n",
    "        words = text.split(' ')\n",
    "        for word in words:\n",
    "            if word in features:\n",
    "                tCount[index] += 1\n",
    "                wordCount[word][index] += 1\n",
    "    outModel = open(modelFile, 'w')\n",
    "    #拉普拉斯平滑\n",
    "    print \"训练完毕，写入模型...\"\n",
    "    for k,v in wordCount.items():\n",
    "        scores = [(v[i]+1) * 1.0 / (tCount[i]+len(wordCount)) for i in xrange(len(v))]\n",
    "        outModel.write(k+\"\\t\"+scores+\"\\n\")\n",
    "    outModel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(modelFile):\n",
    "    '''\n",
    "        从模型文件中导入计算好的贝叶斯模型\n",
    "    '''\n",
    "    print \"加载模型中...\"\n",
    "    f = open(modelFile)\n",
    "    scores = {}\n",
    "    for line in f:\n",
    "        word,counts = line.strip().rsplit('\\t',1)    \n",
    "        scores[word] = eval(counts)\n",
    "    f.close()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(featureFile, modelFile, testText):\n",
    "    '''\n",
    "        预测文档的类标，标准输入每一行为一个文档\n",
    "    '''\n",
    "    docCounts,features = load_feature_words()    \n",
    "    docScores = [math.log(count * 1.0 /sum(docCounts)) for count in docCounts]\n",
    "    scores = load_model(modelFile)\n",
    "    rCount = 0\n",
    "    docCount = 0\n",
    "    print \"正在使用测试数据验证模型效果...\"\n",
    "    for line in testText:\n",
    "        lable,text = line.strip().split('\\t',1)\n",
    "        index = lable2id(lable[0])        \n",
    "        words = text.split(' ')\n",
    "        preValues = list(docScores)\n",
    "        for word in words:\n",
    "            if word in features:                \n",
    "                for i in xrange(len(preValues)):\n",
    "                    preValues[i]+=math.log(scores[word][i])\n",
    "        m = max(preValues)\n",
    "        pIndex = preValues.index(m)\n",
    "        if pIndex == index:\n",
    "            rCount += 1\n",
    "        #print lable,lables[pIndex],text\n",
    "        docCount += 1\n",
    "    print(\"总共测试文本量: %d , 预测正确的类别量: %d, 朴素贝叶斯分类器准确度:%f\" %(rCount,docCount,rCount * 1.0 / docCount)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, math, random, collections\n",
    "def shuffle(inFile):\n",
    "    '''\n",
    "        简单的乱序操作，用于生成训练集和测试集\n",
    "    '''\n",
    "    textLines = [line.strip() for line in open(inFile) if len(line) > 0]\n",
    "    print \"正在准备训练和测试数据，请稍后...\"\n",
    "    random.shuffle(textLines)\n",
    "    num = len(textLines)\n",
    "    trainText = textLines[:3*num/5]\n",
    "    testText = textLines[3*num/5:]\n",
    "    print \"准备训练和测试数据准备完毕，下一步...\"\n",
    "    return trainText, testText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在准备训练和测试数据，请稍后...\n",
      "准备训练和测试数据准备完毕，下一步...\n",
      "体育\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-282-6abee162b3e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrainText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtrainText\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcount_for_cates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_bayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-265-676bd42f315f>\u001b[0m in \u001b[0;36mcount_for_cates\u001b[1;34m(trainText, featureFile)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#扫描文件和计数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainText\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlable2id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    }
   ],
   "source": [
    "inFile = 'data_processed2.json'\n",
    "featureFile = 'feature_file.out'\n",
    "modelFile = 'model_file.out'\n",
    "\n",
    "trainText, testText = shuffle(inFile)\n",
    "print trainText[0]\n",
    "count_for_cates(trainText, featureFile)\n",
    "train_bayes(featureFile, trainText, modelFile)\n",
    "predict(featureFile, modelFile, testText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
