{"cells":[{"cell_type":"markdown","source":["# Preprocess\n\n# 目标：\n\n将数据转换为 label,content 形式的JSON文件\n\n# 问题：\n- 1.数据是XML格式，转换为JSON\n- 2.字符格式是GBK。自己一开始测试时，用UTF-8，Java nio报MalformedInputException\n- 3.数据没有根元素，影响转换，每个文件都要在头和尾，加<docs></docs>\n- 4.url字段，含有特殊符号“&”，影响JSON转换，需要清除该符号\n- 5.有的数据，content字段，是空的，对分词造成影响\n\n# Spark Processing\n\n# 目标：\n\n按Spark ML API要求，组织数据形式\n\n# 步骤：\n\n装载数据，生成dataFrame，相当于数据库表，原始数据格式为(label \\t words),已经分好词，但没有去除停用词\n\n生成dataFrame，对content列做特征提取\n\n转换dataFrame，转换为NaivBayes需要的格式\n\nsplit数据为training and test set\n\n训练贝叶斯分类器\n\n预测\n\n# 问题：\n\n特征提取/选择花费时间长，word2vec有负数，不符合贝叶斯分类的非负数要求，最后，用了最简单的TF-IDF，Spark的TF-IDF平滑于归一化都很自动化，顺便测试了sklearn的TF-IDF，得出的向量不是等维度的\n\n不是太熟悉Transformation操作，需要花点时间测试\n\n数据倾斜问题\n\n互信息应该是更好的特征选择"],"metadata":{}},{"cell_type":"code","source":["#data = sc.textFile('/FileStore/tables/g9pufw5w1475166089797/data_processed2.json')\ndataPath = \"/FileStore/tables/kbhw49a91475214246176/data_processed3.json\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import Row\ndef importJsonFile(file):\n  data = sc.textFile(file)\n  parts = data.map(lambda l: l.split(\"\\t\"))\n  news = parts.map(lambda p: Row(content=p[1].split(\" \"), label=p[0]))\n  return news"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Infer the schema, and register the DataFrame as a table.\ndef newsDataFrame(dataPath):\n    news = importJsonFile(dataPath)\n    newsDF = sqlContext.createDataFrame(news)\n    newsDF.registerTempTable(\"news\")\n    return newsDF"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["newsDataFrame(dataPath)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["result = sqlContext.sql(\"SELECT * FROM news\")\nresult.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["result.groupBy(\"label\").count().show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["result.select(\"content\").take(1)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["'''\n  sklearn TF-IDF feature\n  size of vectors are not the same\n'''\nfrom sklearn.feature_extraction.text import TfidfVectorizer  \ntfidf_transformer = TfidfVectorizer(encoding=u'utf-8',ngram_range=(1, 3),sublinear_tf=True) \nres = result.map(lambda l : [l[1],tfidf_transformer.fit_transform(l[0])])\nres.take(2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["'''\n  Spark Word2Vec feature\n  has negative numbers\n'''\nfrom pyspark.ml.feature import Word2Vec\n\n# Learn a mapping from words to Vectors.\nword2Vec = Word2Vec(vectorSize=1000, minCount=5, inputCol=\"content\", outputCol=\"vector\")\nmodel = word2Vec.fit(result)\nword2vecResult = model.transform(result)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["word2vecResult.registerTempTable(\"word2vecResult\")\nword2vecResult.printSchema()\nword2vecResult.take(1)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["word2vecResult.map(lambda l : [l[1],l[2]]).take(1)#RDD transform test"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["'''\n  Spark TF-IDF feature\n'''\nfrom pyspark.ml.feature import HashingTF, IDF\nhashingTF = HashingTF(inputCol=\"content\", outputCol=\"rawFeatures\", numFeatures=100)\nfeaturizedData = hashingTF.transform(result)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["rescaledData.printSchema()\nrescaledData.select(\"features\").take(2)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["rescaledData.map(lambda l : [l[1],l[3]]).take(1)#RDD transform test"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["'''\n  Prepare for NaiveBayes\n'''\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\nfrom pyspark.mllib.linalg import Vectors\n\ndef parseLine(line):\n  label = float(line[0])\n  features = line[1]\n  return LabeledPoint(label,features)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#norData = word2vecResult.map(lambda row : [row[1],row[2]]).map(parseLine)\nnorData = rescaledData.map(lambda row : [row[1],row[3]]).map(parseLine)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["'''\n  split data into training and test sets\n'''\ntraining, test = norData.randomSplit([0.6, 0.4], seed=0)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["model = NaiveBayes.train(training, 1.0)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Make prediction and test accuracy.\npredictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\naccuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["print accuracy"],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"sogou_news_naive_bayes_classification","notebookId":4050303956693543},"nbformat":4,"nbformat_minor":0}
